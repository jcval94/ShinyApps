library(shiny)
library(shinyjs)
library(forecast)
library(reshape2)
library(TSA)
library(assertthat)
library(tidyverse)
library(randomForest)
library(data.table)
library(cowplot)
library(shinythemes)
##
text_eval<-function(a){
  if(class(a)!="character"){return(invisible())}
  eval(parse(text = a))
}

dtsts<-ls("package:datasets")
classes<-purrr::map(dtsts,~class(text_eval(.x)))
lengs<-purrr::map_int(dtsts,~length(text_eval(.x)))
dtsts1<-dtsts[map_lgl(classes,~"ts" %in% .x) & lengs>30]
##
ui <- fluidPage(
    
    theme = shinytheme("lumen"),
    includeCSS("www/custom.css"),
    
    titlePanel(""),
    br(),
    column(12, offset=0,
           h1(strong("Welcome to this Demo"),style="
				background-image: url(my_image.png);
				opacity: 0.7;
				background-color: white;
				width: 100%;
				text-align:center;
     margin-top: 0px;
     ")),
    br(""),
    mainPanel("", 
              br(),
              column(12, offset=0, align="center" ,
                     
                     style="
				background-image: url(my_image.png);
				opacity: 0.8;
				background-color: white;
				margin-top: -20px;
				width: 150%;
				",br(),
                     # Este anÃ¡lisis surgio al revelar la relacion enre consumo de medicamento y las temporadas anueles
                     # Era necesario ajustar mas rigurosamente la componente estacional de una serie de tiempo, por lo que despues de una ardua investigacion, descubri un articulo en el que se ajustaban series de Fourier con modelos de machine learning.
                     # El algoritmo ajusta las 10 curvas mas aproxiadas de una serie de fourier a una serie de tiempo sustituyendo la componente estacional con las predicciones obtenidas por el ensamble (Random Forest)
                     # Posteriormente realiza tune hypermarametrization, cross validation and seasonality tests.
                     helpText((p("This Time Series Analysis (TSA) arose when revealing the relationship between the medicines consumption and the seasons of annuals. It was necessary to fit more strictly the seasonal component of a series of time. After some research, I discovered an article where Fourier series were used with machine learning models to improve accuracy. I improved that model including seasonality tests, periodograms and a automatable ARIMA model to compare discrepancy. The algorithm fits the best Fourier series curves to a TS seasonal component, substituting this component with the random forest predictions. Subsequently, it performs tune hypermarametrization, cross validation and seasonality tests."
                                 , style="color:black ; font-family: 'times'; font-size:18pt;text-align:left"))) ,
                     
                     br(""),
                     helpText((p("Instructions: Push and wait"
                                 , style="color:black ; font-family: 'times'; font-size:18pt;text-align:left"))) ,
                     
                     selectInput('Tabla', 'Choose a Time Series', dtsts1,selected = "BJsales.lead"),
                     uiOutput("Btn"),
                     br(),
                     h4("It is important to consider the spectral value of a period in a time series model; the closest is to one the more accurate is the period to a series"),
                     plotOutput("Pl1"),
                     br(),
                     h4("Here you can see all the simulations created by fitting the seasonality using a Random Forest model, each line represents a Random Forest with different parameters"),
                     plotOutput("Pl2"),
                     br(),
                     h4("Measuring the accuracy of each Time Series/Machine Learning model it's a key to continue with this analysis, the smallest point in the red plot shows the best possible model, Error used: MAPE (mean absolute percentage error)"),
                     plotOutput("Pl3",height = "750px"),
                     br(),
                     h4("Finally, we can see the prediction (Time Series + Random Forest) vs a second prediction generated by a ARIMA model (the arima model was optimized via Akaike Information Criterion)"),
                     plotOutput("Pl4"),
                     br(),br(),br(),br(),br(),br(),
                     
                     helpText((p("Applications: Financial Forecasting Models  | Pricing | Population growth | Inflation rates | A time series with a seasonal component "
                                 , style="color:black ; font-family: 'times'; font-size:16pt;text-align:left"))) ,
                     
                     h5("Contact: valc941226@gmail.com"),
                     br(),br(),br(),br(),br(),br()
                     
              )
    )
)

# Define server logic required to draw a histogram
server <- function(input, output) {
  
  output$Btn<-renderUI({
    actionButton(inputId = "AnalisisTS",label = "Print Analysis (25 segs)")
  })
  
    observeEvent(input$AnalisisTS,{
        
      output$Btn<-renderUI({
        actionButton(inputId = "AnalisisTS",label = "Print Analysis")
      })
        RFgrid <- function(data_train, param1, param2, K=2, period, period_s2){
            
            if(missing(period)){
                period = Period(data_train)[[1]]
            }
            if(length(period)>1){period<-period[1]}
            
            N <- length(data_train)
            
            perds<-unique(c(period,period_s2))
            data_ts <- msts(data_train, seasonal.periods = perds)
            
            #Se crean variables senoidales y cosenoidales de las estacionalidades
            #de la serie y se generan h periodos a futuro para hacer el test
            #Hago que K est? bien definida
            K<-ifelse(sum(perds/K<2)>0,min(perds[perds/K<2])/K-.01,K)
            
            fuur <- fourier(data_ts, K = rep(K,length(perds)))
            fuur_test <- as.data.frame(fourier(data_ts, K = rep(K,length(perds)), h = max(perds)))
            #El periodo ser? el m?ximo de los periodos tal que divida al train en 2 o m?s segmentos
            
            #period<-min(c(period,period_s2))
            window <- (N / period) - 1
            data_ts <- ts(data_train, freq = period)
            #print("AA")
            decomp_ts <- stl(data_ts, s.window = "per", robust = TRUE)
            #Serie sin tendencia
            new_load <- rowSums(decomp_ts$time.series[, c(1,3)])
            
            #La tendencia se ajusta a un arima, se proyecta n periodos a futuro
            #Y se obtiene la media de dichos periodos
            trend_part <- ts(decomp_ts$time.series[,2])
            trend_fit <- forecast::auto.arima	(trend_part)
            trend_for <- as.vector(forecast(trend_fit, period)$mean)
            per.win<-round(period*window,0)
            #Se obtienen los n-1 periodos del componente estacional
            lag_seas <- decomp_ts$time.series[1:(per.win), 1]
            
            #Se genera el df del train y se crea un RF con ?ste
            matrix_train <- data.frame(Load = tail(new_load, (per.win)),
                                       fuur[(period+1):N,],
                                       Lag = lag_seas)
            #Se explicar? la serie sin tendencia V?a un RF
            tree_2 <- randomForest(Load ~ ., data = matrix_train,
                                   ntree = 80, mtry = param1, nodesize = param2, importance = TRUE)
            #ntree se queda en 730, pues es un n?mero suf. gde para que el modelo haya alcanzado su ntree ?ptimo
            #Trae el complemento de lag_seas
            test_lag <- decomp_ts$time.series[((per.win)+1):N, 1]
            
            #Se genera el df para proyectar
            matrix_test <- data.frame(fuur_test[1:period,],
                                      Lag = test_lag)
            
            pred_tree <- predict(tree_2, matrix_test) + mean(trend_for)
            
            return(as.vector(pred_tree))
        }
        mape <- function(real, pred){
            #print(length(real),length(pred))
            return(mean(abs(1-pred/real)[real!=0])) # MAPE - Mean Absolute Percentage Error
        }
        gridSearch <- function(Y,  FUN, param1, param2, period = Period(Y)[[1]]) {
            param3<-period
            days <- length(Y)#"d?as" u obs. totales ingresadas
            period_s2<-period[2]
            period<-period[1]
            #Usaremos el n?mero de periodos equivalente al .75 de las obs
            #S no alcanza, usaremos lo que resta del periodo
            periodos_a_usar<-max(sum(seq(period,days,by=period)/days<.75),3)
            
            test.days <- days - floor(period)*periodos_a_usar #longitud de observaciones con las que entrenar?
            if(test.days==0){
                param3<-period<-floor(period/2)
                test.days <- days - floor(period)*periodos_a_usar
            }
            #matriz de PAR?Metros 
            mape.matrix <- matrix(0, nrow = length(param1), ncol = length(param2))
            row.names(mape.matrix) <- param1
            colnames(mape.matrix) <- param2
            #Vector de predicciones que ir? llenandose
            #vector(length = test.days*period)
            
            #Hiperparametrizaci?n del bosque
            frcst<-list(p1=c(),p2=c(),p3=c(),p4=list())
            n<-0
            for(i in seq_along(param1)){
                for(j in seq_along(param2)){
                    forecast.rf <- c()
                    for(k in 0:floor(test.days/period)){
                        train.set <- Y[((period*k)+1):((period*k)+(periodos_a_usar*floor(period)))]
                        if(period/length(train.set)>.5){
                            break()
                        }
                        n<-n+1
                        forecast.rf <- c(forecast.rf,FUN(data_train = train.set, param1 = param1[i], param2 = param2[j],
                                                         period=period,period_s2=period_s2))
                        
                        frcst[[1]]<-c(frcst[[1]],i)
                        frcst[[2]]<-c(frcst[[2]],j)
                        frcst[[3]]<-c(frcst[[3]],k)
                        frcst[[4]][[n]]<-forecast.rf
                        
                    }
                    #Real vs predicci?n
                    #print(length(forecast.rf))
                    real<-Y[-(1:(periodos_a_usar*floor(period)))]
                    #print(len(real))
                    mape.matrix[i,j] <- mape(real, forecast.rf[1:length(real)])
                }
            }
            return(list(mape.matrix,param3,frcst))
        }
        RFrep<-function(all_data, win1, win2,period,n.pred){
            #Replicamos el modelo
            period2<-period[1:min(4,length(period))]
            period_s2<-period[1]
            period<-period[2]
            
            repr<-floor(n.pred/period)+2
            forec<-c()
            all_data_t<-all_data
            
            #Agrego la predicci?n a la serie y repito el ejercicio de predicci?n
            for(r in 1:repr){
                forec<-c(forec,RFgrid(all_data_t,win1,win2,2,period,period_s2))
                all_data_t<-c(all_data_t,forec)
                #print("inns")
                #Retiramos los primeros periodos, sino se ir? haciendo m?s lenta
                all_data_t<-all_data_t[-(1:period+((r-1)*period))]
            }
            return(forec[1:n.pred])
        }
        
        Period<-function(ts,place=10){
            
            ddT<-data.frame(freq=c(),spec=c(),orden=c())
            ords<-floor(length(ts)*.7):length(ts)
            
            for(lu in ords){
                #Se identifica estacionalidad semanal si la hay 
                p<-TSA::periodogram(ts[1:lu],plot=F)
                
                #spectrum(ts.anual,log="no")
                dds<- data.frame(freq=1/p$freq, spec=p$spec,orden=1:length(p$spec))
                dds<-head(dds[order(-dds$spec),],place)
                
                ddT<-rbind(ddT,dds)
            }
            
            ddT<-ddT[order(-ddT$spec),]
            Maxi<-max(ddT$spec)
            ddT<-head(ddT[ddT$orden>2,],15)
            
            ddT$Freq_Orden<-paste0(ddT$freq,"_",ddT$orden)
            ddT<-suppressWarnings(reshape2::dcast(ddT,Freq_Orden~.,max,value.var="spec"))
            ddT$.<-ddT$./Maxi
            ddT<-ddT[order(-ddT$.),]
            
            return(list(unique(as.numeric(do.call("rbind",strsplit(ddT$Freq_Orden,"_"))[,1])),ddT))
        }
        ARIMA_M<-function(Vec,n=9,Tend=F){
            #decomponer el vector en trimestres
            freq = Period(Vec)[[1]]
            freq<-max(freq[freq<length(Vec)/2])
            
            if(length(freq)==0){
                freq<-12
                # return(NULL)
                # break("")
            }
            
            dec<-stl(ts(Vec,freq=freq),s.window = "per")$time.series
            #extraemos la parte de tendencia
            dec_t <- dec[,2]
            #Ajusta un arima a la tendencia
            if(Tend){
                AA<-forecast::auto.arima(dec_t)
            }
            else{
                AA<-forecast::auto.arima(rowSums(dec[,1:3]))
            }
            ARIMA<-AA
            
            #Si el modelo es (0,0,0), se puede elegir que tenga al menos un PAR?Metro.
            if(sum(purrr::map_int(AA$model[1:3],length))==0){
                MM<-data.frame()
                nuu<-0
                errores<-c()
                for (p in 0:4) {
                    for (i in 0:1) {
                        for (q in 0:4) {
                            if(sum(p,i,q)>=sumaparm){
                                suppressWarnings(Err<-try(MM<-rbind(MM,data.frame(p=p,i=i,q=q,
                                                                                  AIC=arima(dec_t,c(p,i,q))$aic,
                                                                                  Likehood=arima(dec_t,c(p,i,q))$loglik)),
                                                          silent = T))
                                nuu<-nuu+1
                                if(is.error(Err)){
                                    errores<-c(errores,num)
                                    errores<-unique(errores)
                                    next()}
                            }
                        }
                    }
                }
                #Retiramos los que arrojaron error
                if(length(errores)>0 & sum(errores)!=0){MM<-MM[-unique(errores),]}
                
                #Guarda aquel con menor AIC
                MM[MM$AIC==(min(MM$AIC)),]->BST
                BST<-BST[1,]
                #AIC es nuestro criterio
                ARIMA<-arima(dec_t,c(BST$p,BST$i,BST$q))
            }
            
            #a partir de cu?l comienza
            qt<--(1:(length(Vec)%%freq))
            estac<-(rep(dec[,1][1:freq],floor(n/freq)+2)[qt])[1:n]
            
            #Modelo ARIMA + componente estacional+media del reminder
            Err_ARIMA<-try(pobla<-(as.numeric(predict(ARIMA,n.ahead = n)$`pred`))+estac+mean(dec[,3]),silent = T)
            
            if(assertthat::is.error(Err_ARIMA)){
                pobla<-(as.numeric(forecast::forecast(ARIMA,n)$mean))+estac+mean(dec[,3])
            }
            
            return(list(pobla,ARIMA))
        }
        Evaluate<-function(X,npred=floor(length(X)/4)){
            X<-ifelse(is.nan(X),1,X)
            if(length(X)<15){
                return("Length data must be > 15")
            }
            p<-Period(X)
            
            #Descartamos las Periodes tal que 
            desc<-length(X)/p[[1]]<3
            #si no, no se podr? hacer la descomposici?n
            p[[1]]<-unique(round(p[[1]][!desc],0))
            p[[1]]<-p[[1]][p[[1]]!=2]
            
            if(length(p[[1]])==1 && p[[1]]!=12){p[[1]]<-c(p[[1]],12)}else{p[[1]]<-c(p[[1]],6)}
            #Si no queda ninguno, no tiene sentido ejecutar este m?todo, se deber?n
            #conseguir m?s datos
            
            mp_2<-c()
            res_P<-res_T<-list()
            all_data <- X
            for (i in 1:min(4,max(length(p[[1]])-1,1))){
                #Buscaremos aquel modelo que arroje el menor de los errores
                #En caso de que la Period abarque 2 o menos 
                per<-p[[1]][1:2+(i-1)]
                E1<-try(res_1 <- gridSearch(Y = all_data, FUN = RFgrid, param1 = c(2,3,4,5), param2 = c(2,3,4,5),period = per),silent=T)
                
                #De los periodos obtenidos elegir el de menor error
                if(is.error(E1) | any(is.na(E1[[1]]))){next()}
                res_P[[i]]<-res_1[[3]]
                res_1[[3]]<-NULL
                res_T[[i]]<-res_1[!is.null(res_1)]
                data_grid <- (reshape2::melt(res_1[[1]]))
                #Dado que esta fue la mejor parametrizaci?n, ser? la que usaremos
                winners<-as.numeric(data_grid[data_grid$value==min(data_grid$value),1:2])
                mp_2[i]<-(min(data_grid$value))
                print(mp_2[i])
            }
            print("Fin Fases")
            res_Win<-res_T[ifelse(is.na(mp_2==min(mp_2,na.rm = T)),F,mp_2==min(mp_2,na.rm = T))][[1]]
            
            period<-res_Win[[2]]
            if(length(period)==1){period<-c(period,period+1)}
            data_grid <- (reshape2::melt(res_Win[[1]]))
            winners<-as.numeric(data_grid[data_grid$value==min(data_grid$value),1:2])
            # print(period)
            # print(npred)
            # print(winners)
            pr_p<-RFrep(all_data,winners[1],winners[2],period = period,n.pred=npred)
            
            if(T){
                print("Fin Fases4")
                ddd<-data.frame(Freq=round(map_dbl(strsplit(p[[2]][[1]],"_"),~as.numeric(.x[1])),0),Estim=p[[2]][[2]])
                ddd2<-reshape2::dcast(data = ddd,formula = Freq~.,value.var = "Estim",fun.aggregate = mean)
                ddd2<-ddd2[order(-ddd2[[2]]),];ddd2[[1]]<-as.factor(ddd2[[1]])
                PLT<-ggplot(data=ddd2, aes(x=Freq, y=.)) +
                    geom_bar(stat="identity", color="blue", fill="white")+ylab("Spec")
                
            }
            
            return(list(pr_p,mp_2,res_T,PLT,res_P))
        }
        
        #Empezamos con los ejemplos:
        
        X = as.numeric(get(input$Tabla))
        #Inputar nas
        for(i in 1:(length(X)-3)){
            if(is.na(X[i])){
                X[i]<-mean(X[i:(i+3)],na.rm=T)
            }
        }
        
        
        if(length(X)>250){X<-tail(X,250)}
        
        Predicciones<-Evaluate(X)
        
        X<-X[(length(X)-min(95,length(X)-1)):(length(X))]
        DF<-data.frame(Period=1:length(X),Value=as.numeric(X))
        DF[["CL"]]<-factor("Real",levels = c("Real","Pred"))
        
        DF2<-rbind(data.frame(Period=(length(X)),Value=tail(X,1),CL="Pred"),
                   data.frame(Period=(length(X)+1):(length(X)+length(Predicciones[[1]])),Value=Predicciones[[1]],CL="Pred")
        )
        
        DF<-rbind(DF,DF2)
        
        #Ahora nos podemos dar el lujo de graficar cada parte del proceso
        
        #View del periodograma
        output$Pl1<-renderPlot(Predicciones[[4]])
        
        #Primero un View de los Random Forest
        Ploteos<-Predicciones[[5]][[length(Predicciones[[5]])]]$p4
        
        Lng<-map_int(Ploteos,length)
        Time<-PP<-c()
        for (i in 1:length(Lng)){
            PP<-c(PP,paste0("Pred",rep(i,Lng[i])))
            Time<-c(Time,seq(1,Lng[i]))
        }
        Plot_ss<-data.frame(Pred=PP,Values=do.call(c,Ploteos),Time=Time)
        Plot_sim<-ggplot(Plot_ss,aes(Time,Values,group=Pred))+geom_line()
        
        output$Pl2<-renderPlot(Plot_sim)
        
        #View de los errores
        Grids<-Predicciones[[3]]
        plot_season<-list()
        for(i in 1:length(Grids)){
            res_1<-Grids[[i]][[1]]
            res_2<-Grids[[i]][[2]]
            data_grid <- data.table::data.table(melt(res_1))
            colnames(data_grid) <- c("mtry", "nodesize", "MAPE")
            
            
            plot_season[[i]]<-ggplot(data_grid, aes(mtry, nodesize, size = MAPE, color = MAPE)) +
                geom_point() + ggtitle(paste0("R. Forest params/seasonality: ",paste0(res_2,collapse = ", "))) + 
                scale_color_distiller(palette = "Blues")
            
            if(i==(1:length(Predicciones[[2]]))[Predicciones[[2]]==min(Predicciones[[2]])]){
                plot_season[[i]]<-plot_season[[i]]+scale_color_distiller(palette = "Reds")
            }
        }
        
        CP<-try(cowplot::plot_grid(plot_season[[1]],plot_season[[2]],
                                   plot_season[[3]],plot_season[[4]],nrow=2),silent = T)
        if(is.error(CP)){
            CP<-try(cowplot::plot_grid(plot_season[[1]],plot_season[[2]],
                                       plot_season[[3]],nrow=2),silent = T)
        }
        if(is.error(CP)){
            CP<-try(cowplot::plot_grid(plot_season[[1]],plot_season[[2]],nrow=2))
        }
        if(is.error(CP)){
            CP<-plot_season[[1]]
        }
        output$Pl3<-renderPlot(CP)
        
        Predicciones2<-ARIMA_M(X,n = nrow(DF2))
        
        DFP2<-rbind(DF,rbind(data.frame(Period=(length(X)),Value=tail(X,1),CL="Pred2"),
                             data.frame(Period=(length(X)+1):(length(X)+length(Predicciones2[[1]])),Value=Predicciones2[[1]],CL="Pred2")))
        
        Preds<-ggplot(DFP2,aes(x = Period, y = Value,color = CL), size = 1)+
            geom_line()+
            theme_minimal()+
            scale_color_manual(values = c("#00AFBB", "#E7B800","#A1B900"))
        
        output$Pl4<-renderPlot(Preds)
        
    })
}

# Run the application 
shinyApp(ui = ui, server = server)
